{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1bc3f0f-01de-4b1e-b7e7-aa05a2f09afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "dbutils.widgets.text(\"source_system\", \"SHAREPOINT_HIST\")\n",
    "SOURCE_SYSTEM = dbutils.widgets.get(\"source_system\")\n",
    "\n",
    "files = (\n",
    "    spark.table(\"tp_finance.raw.sharepoint_workbook_files\")\n",
    "      .where(F.col(\"source_system\") == SOURCE_SYSTEM)\n",
    "      .select(\"source_system\", \"source_path\", \"file_sha256\", \"source_modified_ts\", \"load_date\")\n",
    ")\n",
    "\n",
    "parsed = (\n",
    "    spark.table(\"tp_finance.audit.parsed_files\")\n",
    "      .where(F.col(\"source_system\") == SOURCE_SYSTEM)\n",
    "      .select(\"file_sha256\").distinct()\n",
    ")\n",
    "\n",
    "to_parse = files.join(parsed, on=\"file_sha256\", how=\"left_anti\")\n",
    "print(\"Files to parse:\", to_parse.count())\n",
    "\n",
    "if to_parse.count() == 0:\n",
    "    print(\"No new files to parse. Exiting.\")\n",
    "else:\n",
    "    paths = [r[\"source_path\"] for r in to_parse.select(\"source_path\").distinct().collect()]\n",
    "\n",
    "    binary = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "          .load(paths)\n",
    "          .select(F.col(\"path\").alias(\"source_path\"), F.col(\"content\").alias(\"content_bytes\"))\n",
    "    )\n",
    "\n",
    "    work = (to_parse.join(binary, on=\"source_path\", how=\"inner\")\n",
    "                  .select(\"source_system\",\"source_path\",\"file_sha256\",\"load_date\",\"source_modified_ts\",\"content_bytes\"))\n",
    "\n",
    "    schema = T.StructType([\n",
    "      T.StructField(\"source_system\", T.StringType()),\n",
    "      T.StructField(\"source_path\", T.StringType()),\n",
    "      T.StructField(\"file_sha256\", T.StringType()),\n",
    "      T.StructField(\"sheet_name\", T.StringType()),\n",
    "      T.StructField(\"row_num\", T.IntegerType()),\n",
    "      T.StructField(\"row_json\", T.StringType()),\n",
    "      T.StructField(\"load_date\", T.DateType()),\n",
    "      T.StructField(\"source_modified_ts\", T.TimestampType()),\n",
    "      T.StructField(\"ingestion_run_id\", T.StringType()),\n",
    "      T.StructField(\"load_ts\", T.TimestampType()),\n",
    "    ])\n",
    "\n",
    "    def parse_excel_batches(iterator):\n",
    "        from openpyxl import load_workbook\n",
    "        import uuid\n",
    "        from datetime import datetime\n",
    "        import io, json\n",
    "        import pandas as pd\n",
    "\n",
    "        for pdf in iterator:\n",
    "            run_id = str(uuid.uuid4())\n",
    "            now = datetime.utcnow()\n",
    "\n",
    "            out_rows = []\n",
    "\n",
    "            for row in pdf.itertuples(index=False):\n",
    "                try:\n",
    "                    wb = load_workbook(filename=io.BytesIO(row.content_bytes), data_only=True, read_only=True)\n",
    "\n",
    "                    for sheet_name in wb.sheetnames:\n",
    "                        ws = wb[sheet_name]\n",
    "                        it = ws.iter_rows(values_only=True)\n",
    "\n",
    "                        header = next(it, None)\n",
    "                        if not header:\n",
    "                            continue\n",
    "                        header = [str(h).strip() if h is not None else \"\" for h in header]\n",
    "\n",
    "                        row_num = 1\n",
    "                        for vals in it:\n",
    "                            row_num += 1\n",
    "                            d = {}\n",
    "                            for k, v in zip(header, vals):\n",
    "                                if not k:\n",
    "                                    continue\n",
    "                                if hasattr(v, \"isoformat\"):\n",
    "                                    d[k] = v.isoformat()\n",
    "                                else:\n",
    "                                    d[k] = v\n",
    "\n",
    "                            out_rows.append((\n",
    "                                row.source_system,\n",
    "                                row.source_path,\n",
    "                                row.file_sha256,\n",
    "                                sheet_name,\n",
    "                                row_num,\n",
    "                                json.dumps(d, default=str),\n",
    "                                row.load_date,\n",
    "                                row.source_modified_ts,\n",
    "                                run_id,\n",
    "                                now\n",
    "                            ))\n",
    "                except Exception as e:\n",
    "                    # For now: skip file; later write FAILED rows to audit\n",
    "                    continue\n",
    "\n",
    "            # yield a pandas.DataFrame (not return a list/str)\n",
    "            yield pd.DataFrame(out_rows, columns=[f.name for f in schema.fields])\n",
    "\n",
    "    rows_df = work.mapInPandas(parse_excel_batches, schema=schema)\n",
    "\n",
    "    # Write rows (donâ€™t count() first; write triggers execution and is safer)\n",
    "    rows_df.write.mode(\"append\").saveAsTable(\"tp_finance.raw.sharepoint_sheet_rows\")\n",
    "\n",
    "    # Mark parsed only after write succeeds\n",
    "    (\n",
    "      to_parse.select(\n",
    "          F.lit(SOURCE_SYSTEM).alias(\"source_system\"),\n",
    "          \"file_sha256\",\n",
    "          \"source_path\",\n",
    "          F.current_timestamp().alias(\"parsed_ts\"),\n",
    "          F.lit(\"SUCCESS\").alias(\"status\"),\n",
    "          F.lit(None).cast(\"string\").alias(\"error\")\n",
    "      )\n",
    "      .write.mode(\"append\")\n",
    "      .saveAsTable(\"tp_finance.audit.parsed_files\")\n",
    "    )\n",
    "\n",
    "    print(\"Parsing + write complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f54342b-d8af-4ba8-9762-2fe751fda2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "SOURCE_SYSTEM = \"SHAREPOINT_HIST\"\n",
    "\n",
    "files = spark.table(\"tp_finance.raw.sharepoint_workbook_files\").where(F.col(\"source_system\")==SOURCE_SYSTEM)\n",
    "print(\"files:\", files.count())\n",
    "files.select(\"source_path\",\"file_sha256\").show(truncate=False)\n",
    "\n",
    "parsed = spark.table(\"tp_finance.audit.parsed_files\").where(F.col(\"source_system\")==SOURCE_SYSTEM)\n",
    "print(\"parsed:\", parsed.count())\n",
    "\n",
    "to_parse = files.join(parsed.select(\"file_sha256\").distinct(), on=\"file_sha256\", how=\"left_anti\")\n",
    "print(\"to_parse:\", to_parse.count())\n",
    "to_parse.select(\"source_path\",\"file_sha256\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "openpyxl==3.1.*"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sheets_to_rows_sharepoint",
   "widgets": {
    "source_system": {
     "currentValue": "SHAREPOINT_HIST",
     "nuid": "e3b2cafc-a84a-4053-b2a1-61f4746b7bb9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "SHAREPOINT_HIST",
      "label": null,
      "name": "source_system",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "SHAREPOINT_HIST",
      "label": null,
      "name": "source_system",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
